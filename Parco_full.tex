\documentclass{IOS-Book-Article}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{mathptmx}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{ulem}
\usepackage{setspace}
\usepackage{mathptmx}
\usepackage{listings}
\usepackage[dvipdfmx]{color}
\pagestyle{empty} 
\def\hb{\hbox to 10.7 cm{}}
\usepackage{here}
\usepackage{fancybox,ascmac}


\def\keywords#1{\begin{center}{\bf Keywords}\\{#1}\end{center}} 
\begin{document}
\pagestyle{plain}
%\def\thepage{}

\begin{frontmatter} 
\title{ Acceleration of Interactive Multiple Precision Arithmetic Toolbox MuPAT using FMA,  SIMD, and OpenMP }
\author[A]{{\fnms{Hotaka} \snm{YAGI}}
\thanks{Corresponding Author: Graduate student in Department of Applied Mathematics, 1-3 Kagurazaka, Shinjuku-ku, Tokyo 162-8601, Japan; E-mail: 1419521@ed.tus.ac.jp.}},
\author[A]{\fnms{Emiko} \snm{ISHIWATA}},
 and 
\author[B]{ \fnms{Hidehiko} \snm{HASEGAWA}}

\address[A]{Tokyo University of Science, Japan}
\address[B]{ University of Tsukuba, Japan}

\begin{abstract}
MuPAT, an interactive multiple precision arithmetic toolbox for use on MATLAB and Scilab enables the users to treat quadruple and octuple precision arithmetics. MuPAT uses DD and QD algorithm that require from 10 to 600 double precision floating-point operations for each operation. That causes taking much time for computation. In order to reduce the execution time in basic matrix and vector operations, FMA, AVX2 and OpenMP are used. We applied these features to MuPAT using the function of MATLAB executable file, and analyzed the performance. Using AVX2 and OpenMP to unit stride memory references and to avoid synchronization can increase the performance of DD operations. We confirm that the effectiveness of AVX2 and OpenMP is depending on the operational intensity. 
\end{abstract}
\begin{keyword}
Double-Double\sep Quad-Double\sep MATLAB\sep  AVX2\sep Multicore
\end{keyword}
\end{frontmatter}
\section{Introduction TODO.高精度演算の意義を足す(査読4)}

In floating-point arithmetic, rounding error is unavoidable. The accumulation of rounding errors leads to unreliable and inaccurate results. One of the ways to reduce the rounding errors is the use of high-precision arithmetic.  
Most of high-precision arithmetics are accomplished by software emulation such as QD library \cite{QD}. 

Our team developed $MuPAT$, an open-source interactive $Multiple$ $Precision$ $Arithmetic$ $Toolbox$ \cite{saito, hota} for use with the MATLAB and Scilab computing environments. MuPAT uses DD (Double-Double) \cite{DD} and QD (Quad-Double) \cite{QD,DD} algorithms which are based on the combination of double precision arithmetic, and excessively long computation times often result. 
DD and QD arithmetics are possible to accelerate by applying FMA \cite{SIMD}, AVX2 \cite{SIMD}, and OpenMP \cite{omp}. A floating-point multiply-add operation is performed in one step with a single rounding by FMA, four double precisions of data are processed at once with AVX2 instruction, and OpenMP enables thread-level parallelism in a shared memory. 
We implement the combination of these features, and precisely examine some basic matrix and vector operations for DD arithmetic. 


%The performance of matrix and vector operations are increased by the respective way, as discussed in Section 4. The execution time of DD arithmetic can be reduced because the operational intensity of matrix and vector operations are high. We discuss the way to use AVX2 and OpenMP for DD arithmetic to use for MuPAT. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{DD Arithmetic TODO.DD演算の説明足す(査読1, 3), FMAの説明足す(査読2)}

DD (Double-Double) arithmetic \cite{DD} is based on the algorithm that enables quasi quadruple precision arithmetic.
A DD number is defined by two double precision numbers. A DD vector is defined by two vectors with structure of arrays.
DD algorithm requires 10 to 30 of double precision operations for each operation and has data dependency of flow, i.e. the order of computation must be kept. The accuracy of this algorithm consists of the order of the computation. 

DD multiplication algorithm \cite{QD} can utilize FMA (Fused Multiply-Add). 
FMA is a floating-point units that can execute double precision multiply-and-add operation in one instruction. Since a double precision multiply-and-add operation is performed in one step via a single rounding FMA instructions, the rounding error is reduced. 
The algorithms for DD addition and multiplication are shown in Figure \ref{fig1}.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=7cm]{20190731algo.eps}
    \caption{DD addition and multiplication using FMA. The symbols $\oplus$, $\ominus$, $\otimes$ mean the floating-point operators performed on computer, and the symbols $+$, $-$, $\times$ mean mathematical operators. 
$fl(a\times b+c)$ means FMA. TODO.inputとoutputが何かを明確にする(査読1)}
\label{fig1}
  \end{center}
\end{figure}

The number of floating-point operations for DD multiplication is 7 with FMA and 24 without FMA.
That for DD addition is 11. DD addition in Figure \ref{fig1} is called Cray-style \cite{ichi}. Highly accurate, more expensive (20 times of floating-point operations) algorithm is called IEEE-style \cite{ichi}. Cray-style is mainly used, because it is faster \cite{DDBLAS}. TODO. IEEE-styleの説明をするなら詳しくする(査読1)

We measured the execution time of DD multiplication and multiply-and-add that are repeated $10^7$ times using FMA.
We used Intel Core i7 7820HQ, 2.9 GHz CPU and Intel compiler 18.0.3 with options -O2, -fma, -mavx, -fopenmp, and -fp-model precise. 

The execution time of DD multiplication is 0.018 sec, and that of multiply-and-add is 0.048 sec. The ratio for execution time (0.048/0.018 = 2.67) and the ratio for the floating-point operations (18/7 = 2.57) is almost the same. The number of floating-point operations determine the execution time for scalar operation.
%The performances of DD multiplication and multiply-and-add are about 3.88 Gflops/sec and 3.75 Gflops/sec which correspond to 67\% and 65\%, respectively, comparing with the peak computational performance of 5.8 Gflops/sec.
%Since this is an ideal state because of no memory references, 65\% of the computational performance is the target when evaluating the performance in the following sections. We assumed to use FMA instructions for DD arithmetic.

\section{Performance Prediction TODO.AVX2の説明足す(査読2), 実験環境を強調?(査読3)}
Since the order of computation in DD arithmetic can not be changed, we consider to process multiple data simultaneously for parallelization.
The unit time of one operation is not changed, but if multiple results can be obtained in one unit time, then the total execution time is reduced.
We examine to accelerate for basic matrix and vector operations.
AVX2 instructions \cite{SIMD} can perform the single instruction for four double precision of data. The computational performance increases four times, not the memory performance.
OpenMP \cite{omp} can be used for a multicore environment. The memory performance and the computational performance per each core do not change, but it enables to process using multicore. For nested-loop operations, we should discuss which loops to parallelize. 
%Note that the performance (= the number of floating-point operations / execution time [Gflops/sec]) is distinguished between the computational performance [Gflops/sec] and memory performance [Gbytes/sec].

We assume the memory references should be the column order, since MATLAB stores data in column wise. Unit stride memory reference can use the data read by cache line of length of 64 bytes; however, when reading data $N$-stride ($N$ is longer than the length of cache line), each data is read from memory to the register \cite{HPC}. 
%When this is used for unit stride memory references, data transfer between cache and register can be reduced. Although it is slightly faster than utlizing cache line, the speedup for memory references is small. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The performance (= the number of floating-point operations / the execution time [Gflops/sec]) is determined by the memory performance [Gbytes/sec] and the computational performance [Gflops/sec].
For the experiment environment, in the case of single core, the peak computational performance is 5.8 Gflops/sec because there are two FPUs. It increases to 23.2 Gflops/sec by using AVX2. In the case of four cores with AVX2, it is 92.8 Gflops/sec.
The peak memory performance is 34.1 Gbytes/sec, because LPDDR3-2133 can handle 8-bytes data with 2,133 MHz of memory clock frequency and two channels. 

Table \ref{opeData} shows the number of floating-point operations, memory references and operational intensity \cite{roof} which is floating-point operations per memory references. By comparing the ratio of the computational performance for the memory performance and operational intensity, the bottleneck of the application can be predicted \cite{roof}. When operational intensity is higher than 0.17 flops/bytes (5.8/34.1) for single core processing, the execution time is bounded on the time for computation. When lower than 0.17, the execution time is bounded on the time for data transfer. 
In processing of a single core without parallelization, the problem of DD arithmetic is not the number of memory references, but the number of floating-point operations, as shown in Table \ref{opeData}.

\begin{table}[htbp]
\centering
\footnotesize
\caption{The number of floating-point operations [flops], the number of memory references [bytes], and operational intensity [flops/bytes] for DD arithmetic. Let $ \alpha \in \mathbb{R},\ \bm{x},\ \bm{y},\ \bm{z}\in \mathbb{R}^{N},\ A,\ B,\ C \in \mathbb{R}^{N\times N}$.}
\label{opeData}
\begin{tabular}{l||r|r|r}
\hline
    & \multicolumn{1}{c}{floating-point operations}   & \multicolumn{1}{|c|}{memory references}  & \multicolumn{1}{c}{operational intensity}                                    \\ \hline \hline
$\bm{y} = \alpha\bm{x}$ & $7N^{\ } $     & $2N \times 2$$\times$8      &7/32  $ \simeq$	0.22       \\ \hline
$\bm{z} = \bm{x} + \bm{y}$ & $11N^{\ } $     & $3N \times 2$$\times$8   &11/48  $\simeq$	0.23            \\ \hline
%$\bm{z} = \alpha\bm{x} + \bm{y}$ & $18N^{\ } $     & $3N \times 2$$\times$8  &18/48  $\simeq$	0.38            \\ \hline
$\alpha  = \bm{x}^T\bm{y}$& $18N^{\ }$    &$ 2N  \times 2$$\times$8           &18/32  $\simeq$	0.56          \\ \hline
$\bm{y} = A\bm{x}$ & $18N^2$ &$ (N^2+2N) \times 2$$\times$8 &18/16 $\simeq$	1.13 \\ \hline
$C = AB$  & $18N^3$  & $3N^2 \times 2$$\times$8    &$O(N)$\\ \hline
\end{tabular}
\end{table}

\section{Experiment for Matrix and Vector Operations in DD arithmetic TODO. グラフ・表に対応した説明に直す}
\subsection{DD Vector Operations}
Vector operations are single loop processing. 
When we compute the inner product with AVX2, we must sum up the four SIMD register elements after the loop, three scalar additions are needed.
In the case of using OpenMP, since we must sum up the $p$ thread elements after the loop, $p-1$ scalar additions are needed.
When using both AVX2 and OpenMP, each thread processes a vector of length $N/p$ using AVX2.
%The workload of loading data from memory is almost equal to AVX2 and OpenMP, because AVX2 loads four data per loading, and each core of OpenMP accesses $N/4$ of data.

For vector of order $N$ = 4,096,000, the performance of $\alpha  = \bm{x}^T\bm{y}$ is 2.27 Gflops/sec for serial computing, 7.73 Gflops/sec for AVX2, and 8.20 Gflops/sec for four threads OpenMP. When using AVX2 and OpenMP, the performance is 14.08 Gflops/sec.
The performance of $\alpha = \bm{x}^T\bm{x}$ is 27 Gflops/sec, which having half of memory references. 
When using AVX2 and OpenMP, the data supply cannot catch up. 
Since both inner products are bounded on the computational performance, they have the same tendency of the performance as 2.32 Gflops/sec for serial computing, 8.68 Gflops/sec for AVX2, and 8.42 Gflops/sec for OpenMP in Figure \ref{xdot}.
%The performance levels is 15\% (14.08/92.8) that is lower than the target of 65\% due to memory references.
For memory bounded operations, the upper bound of performance can be predicted by the roofline model \cite{roof}. The upper bound is the product of memory performance and operational intensity in Table \ref{opeData}, i.e. the performance levels is 74\% (14.08/(34.1*0.56)). 

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=8.1cm]{20191009dot.eps}
    \caption{Performances [Gflops/sec] of inner products $\bm{x}^T\bm{y}$ and $\bm{x}^T\bm{x}$.}
    \label{xdot}
  \end{center}
\end{figure}

For scalar multiplication of vector, vector addition, and axpy, they require to allocate new memory address for output vector before calling outer C function. This process would become degrade the performance level because roofline model do not consider it.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=8.1cm]{20190730vec1.eps}
    \caption{図は資料20190930\_matomeのp13からp16の図でAVX2+OpenMPのケースを追加したもの(y=x+y, z = x+y, y=ax+y, z=ax+y, (y = ax, x = axも追加する)を比較)を載せる、軸も変える}
    \label{xd}
  \end{center}
\end{figure}

Even OpenMP is used, the situation is the same. The operational intensity of axpy is 0.38 and this value is higher than that of scalar multiplication of vector and vector addition. When using AVX2, we can estimate that the upper bound of performance of axpy is higher. For the vector of order $N$ = 4,096,000, the execution time of axpy is 0.033 sec when serial computing. This time is longer than 0.023 sec for that of the vector addition. However, when we use AVX2, the execution time of axpy is 0.023 sec and that of vector addition is 0.022 sec, as almost the same. Since the workload of memory reference is same for these two operations, the time for computation is reduced by AVX2.



\subsection{DD Matrix-Vector Multiplication}
$y_{i} = \sum a_{i,j}x_{j}$ indicates the matrix-vector multiplication. 
Matrix-vector multiplication can be implemented in two ways whether reading memory is in the row order or column order. The algorithm is differed, as shown in Figure \ref{figimpAVX} of PDOT and PB. We use letter P to represent the shape of Panel, and letter B to represent the shape of Block \cite{goto}. Hereafter, the loops in the program is called first, second, and third from the innermost.
\subsubsection{AVX2}
Since matrix-vector multiplication has nested loops, we should consider the way to use AVX2. The four ways of using AVX2 are shown in Figure \ref{figimpAVX}. Letter {\it v} means vector and letter {\it s} means scalar. 
TODO.Figure4を受けての説明を足す(AVX2のloadが使える、使えない)
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=8.4cm]{20191009MVAVX2.eps}
    \caption{Algorithm for using AVX2 to PDOT or PB for $\bm{y} = A\bm{x}$. The number 1, 2, 3, 4 indicates the position of SIMD register. }
    \label{figimpAVX}
  \end{center}
\end{figure}

%\begin{table}[htbp]
%\footnotesize
%\caption{Difference in the instructions of the way to implement AVX2 to $\bm{y} = A\bm{x}$. Within the round brackets represent the number of double precision instructions for an instruction. }
%\footnotesize
%\begin{tabular}{l|c||c|c|c||c|c|c}
%\hline
%        &\# of instructions&PDOT&PDOT$_{\rm{PB}}$ & PDOT$_{\rm{PDOT}}$ &PB& PB$_{\rm{PB}}$ & PB$_{\rm{PDOT}}$              \\ \hline\hline
%        {\it sload} &2   & &     &    & 1 &     &                        \\ 
%        {\it broadcast} &2  & &     &    &   & 1   &                        \\ 
%        {\it vload} &2  & &     &    &   &     & 1                     \\ \hline\hline
%
%
%        {\it sload} &2  &2&     & 4  & 2 &     & 5                  \\ 
%        {\it vload} &2  & &1    & 1  &   & 2   &                            \\   
%        {\it broadcast} &2  & &1    &    &   &     &                            \\      \hline      
%                
%        {\it smuladd} &18 &1&     &    & 1 &     &      \\                         
%        {\it vmuladd} &18 & &1    & 1  &   & 1   &                           \\    
%        {\it sadd}  &11  & &     &    &   &     & 4     \\ 
%        {\it vmul}  &7  & &     &    &   &     & 1                          \\  \hline  
%        
%        {\it sstore} &2  & &     &    & 1 &     & 1                          \\      
%        {\it vstore} &2 & &     &    &   & 1   &                            \\ \hline\hline
%        
%        {\it sadd}  &11  & &     & 3  &   &     &        \\ \hline
%        {\it sstore} &2 &1&     & 1  &   &     &                        \\
%        {\it vstore} &2 & &1    &    &   &     &                       \\  \hline
%\end{tabular}
%\label{maintab}
%\end{table}

%By using $vmuladd$ instruction instead of $smuladd$ instruction, the number of floating-point operations is reduced to 1/4. The computational workload of PDOT$_{\rm{PB}}$, PDOT$_{\rm{PDOT}}$, and PB$_{\rm{PB}}$ is reduced. However, for PDOT$_{\rm{PDOT}}$, the synchronization is needed after the first loop, so overhead of three $sadd$ are needed. and the workload of loading is also reduced. For PB$_{\rm{PDOT}}$, since synchronization ($sadd$) is needed during the first loop, the computational workload is not become 1/4. Without synchronization ($sadd$), $vstore$ can be used. For load, 

Table \ref{maintab} shows the number of instructions in the loops.
There is two advantages in PDOT$_{\rm{PB}}$ and PB$_{\rm{PB}}$, because the number of operations is reduced by using $vmuladd$ instead of $smuladd$, and the workload of loading is reduced by using $vload$ instead of $sload$. Although PDOT$_{\rm{PDOT}}$ can use $vmuladd$, there is no advantages in PDOT$_{\rm{PDOT}}$ and PB$_{\rm{PDOT}}$, because the number of $sload$ is increased and cannot use $vstore$ due to having synchronization ($sadd$). For PB$_{\rm{PDOT}}$, the synchronization ($sadd$) is serious, because it is computed during the first loop.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=8.4cm]{20190814avx2.eps}
    \caption{Performances [Gflops/sec] using AVX2 to PDOT and PB for $\bm{y} = A\bm{x}$.}
    \label{figMV}
  \end{center}
\end{figure}

TODO. 冗長な説明を直す(パワポのp20の説明に直す)
Using AVX2 in the column order as PDOT$_{\rm{PB}}$ and PB$_{\rm{PB}}$ are faster than using AVX2 in the row order as PDOT$_{\rm{PDOT}}$ and PB$_{\rm{PDOT}}$, because reading data in column order can be used $vload$.
For PDOT$_{\rm{PB}}$ and PB$_{\rm{PB}}$, the performance is nearly four times higher than the without using AVX2, because the time for computation is reduced by $vmuladd$. Especially, PB$_{\rm{PB}}$ is more higher because of unit stride memory references. Since PDOT$_{\rm{PDOT}}$ uses $vmuladd$, performance is increased when the order of matrix $N$ = 500. For PB$_{\rm{PDOT}}$, the performance is increased little due to $sload$ and $sadd$ for all of the $N$.
For a problem the execution time is bounded on the time for the computation, the performance can be improved when the number of floating-point operations can be reduced by AVX2 $vmuladd$ instruction without overhead such as using $sadd$ and $sload$ instruction.


%, because there is no optimization to increase data locality such as blocking. 

\subsubsection{AVX2 + OpenMP}
TODO. OpenMPのみのコメントを追加(外側のループにOpenMPを使うこと)
We do not parallelize first loop, because unit stride memory reference is disturbed. When the pragma directive is used for the second loop of DD matrix-vector multiplication of the order $N$ for PDOT, PDOT$_{\rm{PB}}$ and PDOT$_{\rm{PDOT}}$, each thread computes the vector of the order $N/p$ respectively. These vectors are merged into the vector order of $N$ vertically. In this case, synchronization is not required. For PB, PB$_{\rm{PB}}$, and PB$_{\rm{PDOT}}$, each thread computes the partial sum of the vectors of order $N$ respectively. The result is obtained by summing up of partial sums of $p$ worker vectors. This summing up requires $p-1$ vector additions.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=8.4cm]{20190730omp1.eps}
        \caption{Performances [Gflops/sec] using OpenMP, and using AVX2 to PDOT and PB for $\bm{y} = A\bm{x}$.}
    \label{figMM}
  \end{center}
\end{figure}


As shown in Table \ref{MVomp}, the performance is improved by OpenMP, and PDOT and PB using OpenMP individually have better performance than using AVX2 individually to PDOT$_{\rm{PDOT}}$ and PB$_{\rm{PDOT}}$. 
By comparing the ratio of the computational performance by the memory performance and operational intensity, the operational intensity of matrix-vector multiplication is 1.13 and it is lower than 2.7 (=92.8/34.1), we can predict that this operation is bounded on memory performance.
Therefore, even if the amount of computation can be reduced to 1/4 by using AVX2, the execution time can not be reduced due to memory references. 
In fact, when AVX2 is used with OpenMP, the performance do not increase four times, as shown in Table \ref{MVomp}.

For memory bounded operations, the upper bound of performance can predict by the product of memory performance and operational intensity, i.e. the performance levels is 67\% (25.63/(34.1*1.13).



\subsection{DD Matrix-Matrix Multiplication}
TODO. ループのインデックスの順番、どのループをOpenMPで並列化するか。について説明を追加(パワポp24)
AVX2 can be used to the matrix-vector multiplication and the performance is improved when repeating it. The matrix multiplication ($y_{ij}=\sum a_{jk}x_{kj}$) with index {\it j}-{\it k}-{\it i} order structure has no synchronizations and unit stride memory reference.
The result of parallelizing the third loop and the second loop of matrix multiplication of {\it j}-{\it k}-{\it i} order structure for PB$_{\rm{PB}}$ is shown in Figure \ref{figMM3}.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=8.4cm]{20191009mm.eps}
         \caption{Performances [Gflops/sec] using both AVX2 and OpenMP (parallelizing Third loop and Second loop) for $Y = AX$.}
    \label{figMM3}
  \end{center}
\end{figure}

TODO. 一番性能が高いケースの行列積の傾向を表を使って説明し直す(パワポp29)
For the case that matrix of order $N$ = 2,500, the performance of PB$_{\rm{PB}}$ is 42.98 Gflops/sec.
For the order of matrix of order $N$ = 560, the performance is peak, i.e. 57.31 Gflops/sec and the performance levels of 61.76\%. For the serial computing, the performance is 3.56 Gflops/sec.
The performance is from 40 Gflops/sec to 60 Gflops/sec when parallelizing third loop. However, when parallelizing second loop, the performance is from 30 Gflops/sec to 50 Gflops/sec. 
%The gap between peak and the performance for large matrix are increased comparing with without using OpenMP. %For matrix-vector multiplication, this gap is not appeared. The gap is appeared when without applying optimization of increasing data locality such as blocking.

Since, the operational intensity of matrix multiplication is too large as $O(N)$, the execution time is bounded on time for computation. For parallelizing third loop, the performance improve almost theoretically 16 times by utilizing both AVX2 and OpenMP.
For other operations such as vector operations and matrix-vector operations, the performances are not increased theoretically, because their operational intensity is lower than 2.7 (=92.8/34.1). %The improvement of performance by AVX2 and OpenMP is in proportion to operational intensities.

%Unlike in matrix-vector multiplication, without optimization of increasing data locality, performance worsen at large data size, as shown in Figure \ref{figMM3}.
\section{IEEE-style TODO. 主張をまとめ, 別紙で取捨の確認(パワポp46, p47あたり)}

\subsection{Summary TODO. 文章の修正}

Before offloading to outer C function, it is required to allocate memory address in MATLAB. This overhead degrades some vector operations such as vector addition, scalar multiplication of vector, and axpy, which require output of vector. This overhead cannot consider in the roof line model, so the performance levels are low. Since the inner product is not require output of vector, its performance level is high when using the roof line model. However, the performances for vector operations are low , because their operational intensities are low, and being bounded on memory references.

For DD matrix-vector operations that have nested loops, the way to use AVX2 and OpenMP should be considered. In order to achieve high performance by AVX2, it is necessary to avoid synchronization and using AVX2 loading instructions, not the set of scalar loading instructions. Using AVX2 in this way, the number of floating-point operations can be reduced almost 1/4, and the performance can be increased. Since the operational intensity is high as twice as inner product, the performance is increased to almost the upper bound of 85\% when using both AVX2 and OpenMP. 

For DD matrix multiplication, the performance degraded after the peak, using both AVX2 and OpenMP are not enough to optimize. However, when using AVX2 in the effective way, the performance increases almost theoretically and the execution time is reduced, because its operational intensity is so high.

\section{Conclusion TODO. 文書の修正}
In response to demands for ways to facilitate high-precision arithmetic with an interactive computing environment, we developed MuPAT on Scilab/MATLAB. MuPAT uses DD arithmetic that requires large number of floating-point operations. Executing DD arithmetic takes much time for computation. It is possible to offload to outer C function by MATLAB executable file and use FMA, AVX2, OpenMP supported by modern CPU. 

Since FMA can reduce the number of floating-point operations, we assume to use FMA.  
%For vector addition and scalar multiplication of vector, the operational intensities are low and the performance is not increased. Operational intensity of inner product is higher than these two vector operations and the performance levels become almost the upper bound of roofline model.
For matrix-vector multiplication and matrix multiplication, there is some way to use AVX2 and OpenMP. 
For using AVX2, when loading, computing, and storing is executed with four pack of double precision numbers, the performance is increased. 
To execute AVX2 loading, computing, and storing instructions, we should avoid synchronization and avoid using four scalar loading instruction. 
For using OpenMP, we should avoid synchronization and avoid disturbing unit stride memory references.
DD matrix-vector multiplcation and matrix multiplication can be implemented in these way. 
%The performance levels of matrix-vector multiplcation become almost the upper bound of roofline model. That of matrix multiplication is lower, i.e. 60\%. 

The innermost structures of inner products, matrix-vector multiplication, and matrix multiplication are the one multiply-and-add. MuPAT on Scilab is implemented these three operations into one as matrix multiplication routine. Since the way of implementation to increase the performance for these three operations are different, MuPAT on MATLAB is implemented as three different routines. There is the trade-off between high performance program and general purpose program.
%For matrix-vector multiplication and vector operations, the operational intensities are not enough to speed up theoretically when using AVX2 and OpenMP. 

The effectiveness of AVX2 and OpenMP is depending on the operational intensity. 
Although the performance become almost the upper bound, it does not mean that the execution time is reduced theoretically. Reducing the number of memory references and increasing the number of floating-point operations let the operational intensity become high. For DD arithmetic, using large routines such as axpy and using IEEE-style can increase the operational intensity. As a result, the performance can be increased with almost the same execution time before increasing the operational intensity. 

Even the DD arithmetic, matrix and vector operations are bounded on the memory performance except for matrix multiplication. Recent CPUs can hide the computational workload of these operations in DD arithmetic by using FMA, AVX2, and OpenMP. The next problem for some matrix and vector operations in DD arithmetic is the number of memory references. 

\section*{Acknowledgment}

This research was supported by a grant from the Japan Society for the Promotion of Science (JSPS$\colon$JP17K00164).
\begin{thebibliography}{99}
\bibitem{QD} Y. Hida, X. S. Li, and D. H. Baily, QD arithmetic: algorithms, implementation, and application, Technical Report LBNL-46996, Lawrence Berkeley National Laboratory, 2000.

\bibitem{saito} S. Kikkawa, T. Saito, E. Ishiwata, and H. Hasegawa, Development and acceleration of multiple precision arithmetic toolbox MuPAT for Scilab, {\it JSIAM Letters} {\bf 5} (2013), 9-12. 

\bibitem{hota} MuPAT on MATLAB. {\tt https://www.ed.tus.ac.jp/1419521/index.html}

\bibitem{DD} T. J. Dekker, A floating-point technique for extending the available precision, {\it Numerische Mathematik} {\bf 18} (1971), 224-242. 
\bibitem{SIMD} Intel Intrinsics Guide. {\tt https://software.intel.com/sites/landingpage/IntrinsicsGuide/}
\bibitem{omp} L. Dagum, and R. Menon, OpenMP: An Industry-Standard API for Shared-Memory Programming, {\it IEEE Computational Science \& Engineering} {\bf 5} (1998), 46-55. 

\bibitem{ichi}I. Yamazaki, S. Tomov, T. Dong, J. Dongarra, Mixed-precision orthogonalization scheme and adaptive step size for improving the stability and performance of CA-GMRES on GPUs, in: Proceedings of International Meeting on High Performance Computing for Computational Science (VECPAR), 2014, 17-30.

\bibitem{DDBLAS} X. S. Li et al., Design, Implementation and Testing of Extended and Mixed Precision BLAS, {\it ACM Transactions on Mathematical Software} {\bf 28} (2002), 152-205.

\bibitem{HPC} K. Dowd and C. Severance, High Performance Computing, 2nd Edition, {\it O'Reilly}, 1998.

\bibitem{roof} S. Williams, A. Waterman, and D. Patterson, Roofline: An insightful visual performance model for multicore architectures, {\it Communications of the ACM} {\bf 52} (2009),  65-76.

\bibitem{goto} K. Goto, and R. van de Geijn, Anatomy of High-Performance Matrix Multiplication, {\it ACM Transactions on Mathematical Software} {\bf 34} (2008), Article 12 (25 pages).

\end{thebibliography}
\end{document}