\documentclass{IOS-Book-Article}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{mathptmx}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{ulem}
\usepackage{setspace}
\usepackage{mathptmx}
\usepackage{listings}
\usepackage[dvipdfmx]{color}
\pagestyle{empty} 
\def\hb{\hbox to 10.7 cm{}}
\usepackage{here}
\usepackage{fancybox,ascmac}


\def\keywords#1{\begin{center}{\bf Keywords}\\{#1}\end{center}} 
\begin{document}
\pagestyle{plain}
%\def\thepage{}

\begin{frontmatter} 
\title{ Acceleration of Interactive Multiple Precision Arithmetic Toolbox MuPAT using FMA,  SIMD, and OpenMP }
\author[A]{{\fnms{Hotaka} \snm{YAGI}}
\thanks{Corresponding Author: Graduate student in Department of Applied Mathematics, 1-3 Kagurazaka, Shinjuku-ku, Tokyo 162-8601, Japan; E-mail: 1419521@ed.tus.ac.jp.}},
\author[A]{\fnms{Emiko} \snm{ISHIWATA}},
 and 
\author[B]{ \fnms{Hidehiko} \snm{HASEGAWA}}

\address[A]{Tokyo University of Science, Japan}
\address[B]{ University of Tsukuba, Japan}

\begin{abstract}
MuPAT, an interactive multiple precision arithmetic toolbox for use on MATLAB and Scilab enables the users to treat quadruple and octuple precision arithmetics. MuPAT uses DD and QD algorithm that require from 10 to 600 double precision floating-point operations for each operation. That causes taking much time for computation. In order to reduce the execution time in basic matrix and vector operations, FMA, AVX2 and OpenMP are used. We applied these features to MuPAT using the function of MATLAB executable file, and analyzed the performance. Using AVX2 and OpenMP to unit stride memory references and to avoid synchronization can increase the performance of DD operations. We confirm that the effectiveness of AVX2 and OpenMP is depending on the operational intensity. 
\end{abstract}
\begin{keyword}
Double-Double\sep Quad-Double\sep MATLAB\sep  AVX2\sep Multicore
\end{keyword}
\end{frontmatter}
\section{Introduction TODO.高精度演算の意義を足す(査読4)}

In floating-point arithmetic, rounding error is unavoidable. The accumulation of rounding errors leads to unreliable and inaccurate results. One of the ways to reduce the rounding errors is the use of high-precision arithmetic.  
Most of high-precision arithmetics are accomplished by software emulation such as QD library \cite{QD}. 

Our team developed $MuPAT$, an open-source interactive $Multiple$ $Precision$ $Arithmetic$ $Toolbox$ \cite{saito, hota} for use with the MATLAB and Scilab computing environments. MuPAT uses DD (Double-Double) \cite{DD} and QD (Quad-Double) \cite{QD,DD} algorithms which are based on the combination of double precision arithmetic, and excessively long computation times often result. 
DD and QD arithmetics are possible to accelerate by applying FMA \cite{SIMD}, AVX2 \cite{SIMD}, and OpenMP \cite{omp}. A floating-point multiply-add operation is performed in one step with a single rounding by FMA, four double precisions of data are processed at once with AVX2 instruction, and OpenMP enables thread-level parallelism in a shared memory. 
We implement the combination of these features, and precisely examine some basic matrix and vector operations for DD arithmetic. 


%The performance of matrix and vector operations are increased by the respective way, as discussed in Section 4. The execution time of DD arithmetic can be reduced because the operational intensity of matrix and vector operations are high. We discuss the way to use AVX2 and OpenMP for DD arithmetic to use for MuPAT. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{DD Arithmetic TODO.DD演算の説明足す(査読1, 3), FMAの説明足す(査読2)}

DD (Double-Double) arithmetic \cite{DD} is based on the algorithm that enables quasi quadruple precision arithmetic.
A DD number is defined by two double precision numbers. A DD vector is defined by two vectors with structure of arrays.
DD algorithm requires 10 to 30 of double precision operations for each operation and has data dependency of flow, i.e. the order of computation must be kept. The accuracy of this algorithm consists of the order of the computation. 

DD multiplication algorithm \cite{QD} can utilize FMA (Fused Multiply-Add). 
FMA is a floating-point units that can execute double precision multiply-and-add operation in one instruction. Since a double precision multiply-and-add operation is performed in one step via a single rounding FMA instructions, the rounding error is reduced. 
The algorithms for DD addition and multiplication are shown in Figure \ref{fig1}.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=7cm]{20190731algo.eps}
    \caption{DD addition and multiplication using FMA. The symbols $\oplus$, $\ominus$, $\otimes$ mean the floating-point operators performed on computer, and the symbols $+$, $-$, $\times$ mean mathematical operators. 
$fl(a\times b+c)$ means FMA. TODO.inputとoutputが何かを明確にする(査読1)}
\label{fig1}
  \end{center}
\end{figure}

The number of floating-point operations for DD multiplication is 7 with FMA and 24 without FMA.
That for DD addition is 11. DD addition in Figure \ref{fig1} is called Cray-style \cite{ichi}. Highly accurate, more expensive (20 times of floating-point operations) algorithm is called IEEE-style \cite{ichi}. Cray-style is mainly used, because it is faster \cite{DDBLAS}. TODO. IEEE-styleの説明をするなら詳しくする(査読1)

We measured the execution time of DD multiplication and multiply-and-add that are repeated $10^7$ times using FMA.
We used Intel Core i7 7820HQ, 2.9 GHz CPU and Intel compiler 18.0.3 with options -O2, -fma, -mavx, -fopenmp, and -fp-model precise. 

The execution time of DD multiplication is 0.018 sec, and that of multiply-and-add is 0.048 sec. The ratio for execution time (0.048/0.018 = 2.67) and the ratio for the floating-point operations (18/7 = 2.57) is almost the same. The number of floating-point operations determine the execution time for scalar operation.
%The performances of DD multiplication and multiply-and-add are about 3.88 Gflops/sec and 3.75 Gflops/sec which correspond to 67\% and 65\%, respectively, comparing with the peak computational performance of 5.8 Gflops/sec.
%Since this is an ideal state because of no memory references, 65\% of the computational performance is the target when evaluating the performance in the following sections. We assumed to use FMA instructions for DD arithmetic.

\section{Performance Prediction TODO.AVX2の説明足す(査読2), 実験環境を強調?(査読3), Rooflineの図を入れる?(査読3)}
Since the order of computation in DD arithmetic can not be changed, we consider to process multiple data simultaneously for parallelization.
The unit time of one operation is not changed, but if multiple results can be obtained in one unit time, then the total execution time is reduced.
We examine to accelerate for basic matrix and vector operations.
AVX2 instructions \cite{SIMD} can perform the single instruction for four double precision of data. The computational performance increases four times, not the memory performance.
OpenMP \cite{omp} can be used for a multicore environment. The memory performance and the computational performance per each core do not change, but it enables to process using multicore. For nested-loop operations, we should discuss which loops to parallelize. 
%Note that the performance (= the number of floating-point operations / execution time [Gflops/sec]) is distinguished between the computational performance [Gflops/sec] and memory performance [Gbytes/sec].

We assume the memory references should be the column order, since MATLAB stores data in column wise. Unit stride memory reference can use the data read by cache line of length of 64 bytes; however, when reading data $N$-stride ($N$ is longer than the length of cache line), each data is read from memory to the register \cite{HPC}. 
%When this is used for unit stride memory references, data transfer between cache and register can be reduced. Although it is slightly faster than utlizing cache line, the speedup for memory references is small. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The performance (= the number of floating-point operations / the execution time [Gflops/sec]) is determined by the memory performance [Gbytes/sec] and the computational performance [Gflops/sec].
For the experiment environment, in the case of single core, the peak computational performance is 5.8 Gflops/sec because there are two FPUs. It increases to 23.2 Gflops/sec by using AVX2. In the case of four cores with AVX2, it is 92.8 Gflops/sec.
The peak memory performance is 34.1 Gbytes/sec, because LPDDR3-2133 can handle 8-bytes data with 2,133 MHz of memory clock frequency and two channels. 

Table \ref{opeData} shows the number of floating-point operations, memory references and operational intensity \cite{roof} which is floating-point operations per memory references. By comparing the ratio of the computational performance for the memory performance and operational intensity, the bottleneck of the application can be predicted \cite{roof}. When operational intensity is higher than 0.17 flops/bytes (5.8/34.1) for single core processing, the execution time is bounded on the time for computation. When lower than 0.17, the execution time is bounded on the time for data transfer. 
In processing of a single core without parallelization, the problem of DD arithmetic is not the number of memory references, but the number of floating-point operations, as shown in Table \ref{opeData}.

\begin{table}[htbp]
\centering
\footnotesize
\caption{The number of floating-point operations [flops], the number of memory references [bytes], and operational intensity [flops/bytes] for DD arithmetic. Let $ \alpha \in \mathbb{R},\ \bm{x},\ \bm{y},\ \bm{z}\in \mathbb{R}^{N},\ A,\ B,\ C \in \mathbb{R}^{N\times N}$.}
\label{opeData}
\begin{tabular}{l||r|r|r}
\hline
    & \multicolumn{1}{c}{floating-point operations}   & \multicolumn{1}{|c|}{memory references}  & \multicolumn{1}{c}{operational intensity}                                    \\ \hline \hline
$\bm{y} = \alpha\bm{x}$ & $7N^{\ } $     & $2N \times 2$$\times$8      &7/32  $ \simeq$	0.22       \\ \hline
$\bm{z} = \bm{x} + \bm{y}$ & $11N^{\ } $     & $3N \times 2$$\times$8   &11/48  $\simeq$	0.23            \\ \hline
%$\bm{z} = \alpha\bm{x} + \bm{y}$ & $18N^{\ } $     & $3N \times 2$$\times$8  &18/48  $\simeq$	0.38            \\ \hline
$\alpha  = \bm{x}^T\bm{y}$& $18N^{\ }$    &$ 2N  \times 2$$\times$8           &18/32  $\simeq$	0.56          \\ \hline
$\bm{y} = A\bm{x}$ & $18N^2$ &$ (N^2+2N) \times 2$$\times$8 &18/16 $\simeq$	1.13 \\ \hline
$C = AB$  & $18N^3$  & $3N^2 \times 2$$\times$8    &$O(N)$\\ \hline
\end{tabular}
\end{table}

\section{Experiment for Matrix and Vector Operations in DD arithmetic TODO. グラフ・表に対応した説明に直す}
\subsection{DD Vector Operations}
Vector operations are single loop processing. 
When we compute the inner product with AVX2, we must sum up the four SIMD register elements after the loop, three scalar additions are needed.
In the case of using OpenMP, since we must sum up the $p$ thread elements after the loop, $p-1$ scalar additions are needed.
When using both AVX2 and OpenMP, each thread processes a vector of length $N/p$ using AVX2.
%The workload of loading data from memory is almost equal to AVX2 and OpenMP, because AVX2 loads four data per loading, and each core of OpenMP accesses $N/4$ of data.

For vector of order $N$ = 4,096,000, the performance of $\alpha  = \bm{x}^T\bm{y}$ is 2.27 Gflops/sec for serial computing, 7.73 Gflops/sec for AVX2, and 8.20 Gflops/sec for four threads OpenMP. When using AVX2 and OpenMP, the performance is 14.08 Gflops/sec.
The performance of $\alpha = \bm{x}^T\bm{x}$ is 27 Gflops/sec, which having half of memory references. 
When using AVX2 and OpenMP, the data supply cannot catch up. 
Since both inner products are bounded on the computational performance, they have the same tendency of the performance as 2.32 Gflops/sec for serial computing, 8.68 Gflops/sec for AVX2, and 8.42 Gflops/sec for OpenMP in Figure \ref{xdot}.
%The performance levels is 15\% (14.08/92.8) that is lower than the target of 65\% due to memory references.
For memory bounded operations, the upper bound of performance can be predicted by the roofline model \cite{roof}. The upper bound is the product of memory performance and operational intensity in Table \ref{opeData}, i.e. the performance levels is 74\% (14.08/(34.1*0.56)). 

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=10cm]{20191009dot.eps}
    \caption{Performances [Gflops/sec] of inner products $\bm{x}^T\bm{y}$ and $\bm{x}^T\bm{x}$.}
    \label{xdot}
  \end{center}
\end{figure}

For scalar multiplication of vector, vector addition, and axpy, they require to allocate new memory address for output vector before calling outer C function. This process would become degrade the performance level because roofline model do not consider it.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=8.1cm]{20190730vec1.eps}
    \caption{図は資料20190930\_matomeのp13からp16の図でAVX2+OpenMPのケースを追加したもの(y=x+y, z = x+y, y=ax+y, z=ax+y, (y = ax, x = axも追加する)を比較)を載せる、軸も変える}
    \label{xd}
  \end{center}
\end{figure}

Even OpenMP is used, the situation is the same. The operational intensity of axpy is 0.38 and this value is higher than that of scalar multiplication of vector and vector addition. When using AVX2, we can estimate that the upper bound of performance of axpy is higher. For the vector of order $N$ = 4,096,000, the execution time of axpy is 0.033 sec when serial computing. This time is longer than 0.023 sec for that of the vector addition. However, when we use AVX2, the execution time of axpy is 0.023 sec and that of vector addition is 0.022 sec, as almost the same. Since the workload of memory reference is same for these two operations, the time for computation is reduced by AVX2.



\subsection{DD Matrix-Vector Multiplication}
$y_{i} = \sum _i\sum _j a_{ij}x_{j}$, and $y_{i} = \sum _j\sum _i a_{ij}x_{j}$ 
indicate the matrix-vector multiplication. 
Matrix-vector multiplication can be implemented in 
two types of algorithms depending on the order of index $i$ and $j$. We call $i$-$j$ type as PDOT, and $j$-$i$ type as PB.
These algorithms are differ in the memory access. PB is the unit stride access in MATLAB.
We use letter P to represent the shape of Panel, and letter B to represent the shape of Block \cite{goto}. 

\subsubsection{AVX2}
Since matrix-vector multiplication has nested loops, we should consider the way to use AVX2. 
The four ways of using AVX2 are shown in Figure \ref{figimpAVX}. 
Here, the letter {\it v} means vector instruction and letter {\it s} means scalar instruction. 
Blue words are the instructions that can use vector instructions. 
Red words are the instructions that cannot use vector instructions or overheads.
(青文字・赤文字の説明は入れた方が良い？冗長な気も。。)
TODO.Figure4を受けての説明を足す(AVX2のloadが使える、使えない)
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=8.4cm]{20191009MVAVX2.eps}
    \caption{Algorithm for using AVX2 to PDOT or PB for $\bm{y} = A\bm{x}$.}
    \label{figimpAVX}
  \end{center}
\end{figure}

For using AVX2 for the column order as PDOT$_{\rm{PB}}$ and PB$_{\rm{PB}}$, we can use AVX2 loading instruction to load four continuous data of matrix.
For using AVX2 for row order as PDOT$_{\rm{PDOT}}$ and PB$_{\rm{PDOT}}$, we should use four times of scalar loading instruction to load four discontinous data of matrix.
Furthermore, PDOT$_{\rm{PDOT}}$ and PB$_{\rm{PDOT}}$ require overhead for summing up, and they cannot use AVX2 storing instruction.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=10cm]{20191011avx2.eps}
    \caption{Performances [Gflops/sec] using AVX2 to PDOT and PB for $\bm{y} = A\bm{x}$.}
    \label{figMV}
  \end{center}
\end{figure}

TODO. 冗長な説明を直す(パワポのp20の説明に直す)
PDOT$_{\rm{PB}}$ and PB$_{\rm{PB}}$ that can use AVX2 loading and storing instructions are higher in the performance than others.
The performance is increased almost four times by using AVX2 for PDOT$_{\rm{PB}}$ and PB$_{\rm{PB}}$. 

\subsubsection{AVX2 + OpenMP TODO. Inner loop or First loop}
TODO. OpenMPのみのコメントを追加(外側のループにOpenMPを使うこと)
For using OpenMP, there are two ways depending on which loop (outer or inner) to parallelize. 
For PDOT with $N$ = 2,500, the performance for outer loop parallelism is 10.76 Gflops/sec and that for inner loop parallelism is 4.55 Gflops/sec.
For PB with $N$ = 2,500, the performance for outer loop parallelism is 13.07 Gflops/sec and that for inner loop parallelism is 4.79 Gflops/sec.
These result shows that parallelizing the outer loop is better to use OpenMP for this operation. 
The Figure \ref{figMM} shows that the combinations of using AVX2 for column to load four data of matrix continuously and using OpenMP to parallelize the outer loop.
% When the pragma directive is used for the second loop of DD matrix-vector multiplication of the order $N$ for PDOT, PDOT$_{\rm{PB}}$ and PDOT$_{\rm{PDOT}}$, each thread computes the vector of the order $N/p$ respectively. These vectors are merged into the vector order of $N$ vertically. In this case, synchronization is not required. For PB, PB$_{\rm{PB}}$, and PB$_{\rm{PDOT}}$, each thread computes the partial sum of the vectors of order $N$ respectively. The result is obtained by summing up of partial sums of $p$ worker vectors. This summing up requires $p-1$ vector additions.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=10cm]{20191011mv_conc.eps}
        \caption{Performances [Gflops/sec] using OpenMP, and using AVX2 to PDOT and PB for $\bm{y} = A\bm{x}$.}
    \label{figMM}
  \end{center}
\end{figure}


% As shown in Table \ref{MVomp}, the performance is improved by OpenMP, and PDOT and PB using OpenMP individually have better performance than using AVX2 individually to PDOT$_{\rm{PDOT}}$ and PB$_{\rm{PDOT}}$. 
PB$_{\rm{PB}}$ with OpenMP is the best in performance. 
Even using AVX2 and OpenMP, the execution time cannot be reduced 16 times (AVX2 x 4-core) comparing with the serial as shown in Table \ref{tabMV}. 
Since the operational intensity of matrix-vector multiplication is 1.13 and it is lower than 2.7 (=92.8/34.1), matrix-vector multiplication would become being bounded on memory when using AVX2 and OpenMP.
Comparing with the upper bound of performance, the performance levels when $N$ = 2,500 is 66\% (25.57/(34.1*1.13).

\begin{table}[htbp]
  \centering
  \caption{Execution time, Speed-up ratio, Performance, Upper bound of performance, and performance level for PB$_{\rm{PB}}$ and the outer loop parallelize for $N$ = 2,500}
  \label{tabMV}
  \small
  \begin{tabular}{l|rrrrr}
    \hline
                                                        & \begin{tabular}[c]{@{}c@{}}Execution Time\\ {[}sec{]}\end{tabular} & Speed-up & \begin{tabular}[c]{@{}c@{}}Performance\\ {[}Gflops/sec{]}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Upper bound of\\ Performance\\ {[}Gflops/sec{]}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Performance \\ level {[}\%{]}\end{tabular} \\ \hline
  Serial                                                & 0.0316                                                             & 1.00     & 3.55                                                                   & 5.8                                                                                     & 61                                                                    \\ \hline
  AVX2                                                  & 0.0094                                                             & 3.37     & 11.97                                                                  & 23.2                                                                                    & 52                                                                    \\ \hline
  OpenMP                                                & 0.0086                                                             & 3.68     & 13.07                                                                  & 23.2                                                                                    & 56                                                                    \\ \hline
  \begin{tabular}[c]{@{}l@{}}AVX2\\ OpenMP\end{tabular} & 0.0044                                                             & 7.20     & 25.57                                                                  & 38.5                                                                                    & 66                                                                   \\ \hline
  \end{tabular}
  \end{table}



\subsection{DD Matrix-Matrix Multiplication}
TODO. ループのインデックスの順番、どのループをOpenMPで並列化するか。について説明を追加(パワポp24)
The matrix multiplication ($y_{ij}=\sum_{ijk} a_{jk}x_{kj}$) has six types of algorithms depending on the order of index {\it j}-{\it k}-{\it i}.
\begin{table}[htbp]
  \centering
  \caption{Performance of matrix multiplication when $N$ is 2,500 depending on the order of index.}
  \label{mmindx}
  \small
  \begin{tabular}{l|cccccc}
    \hline
  Index of Outer loop          & $i$    & $i$    & $j$    & $j$    & $k$    & $k$    \\
  Index of Inner loop          & $k$    & $j$    & $i$    & $k$    & $j$    & $i$    \\
  Index of Inner most loop     & $j$    & $k$    & $k$    & $i$    & $i$    & $j$    \\
  Performance {[}Gflops/sec{]} &  0.80  &  0.81  &  0.89  &  3.62  &  3.52  & 0.83
  \\ \hline
  \end{tabular}
\end{table}

When the index of inner most loop is $i$, we can use unit stride access in MATLAB and result in higher performance, that is 3.6 Gflops/sec.
When using AVX2 for the coulumn to load four data of matrix continuously, the performance for matrix multiplication of $j$-$k$-$i$ order and that of $k$-$j$-$i$ order are almost the same.
When $N$ = 2,500, the formar is 12.00 Gflops/sec and the latter is 12.27 Gflops/sec.
For using OpenMP, the performance for matrix multiplication with $N$ = 2,500 of $j$-$k$-$i$ order with the outer loop parallelism is 12.26 Gflops/sec
and that of the inner loop parallelism is 11.93 Gflops/sec.
As for the $k$-$j$-$i$ order, the performance of the outer loop parallelism is 6.87 Gflops/sec and the inner loop parallelsm is 11.78 Gflops/sec.
The only case of using OpenMP to the outer loop for $k$-$j$-$i$ order is low performance, and the others are not differ.
However, when using AVX2 and OpenMP, the performance of $j$-$k$-$i$ order with outer loop parallelism is the highest as in Figure \ref{figMM3}. 

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[clip,width=10cm]{20191009mm.eps}
         \caption{Performances [Gflops/sec] using both AVX2 and OpenMP to the outer loop for $j$-$k$-$i$ order and $k$-$j$-$i$ order for $C = AB$.}
    \label{figMM3}
  \end{center}
\end{figure}

TODO. 一番性能が高いケースの行列積の傾向を表を使って説明し直す(パワポp29)
%The gap between peak and the performance for large matrix are increased comparing with without using OpenMP. %For matrix-vector multiplication, this gap is not appeared. The gap is appeared when without applying optimization of increasing data locality such as blocking.
Table \ref{tabMM} shows the case of $j$-$k$-$i$ order with AVX2 for the column and OpenMP for the outer loop. 
Since the operational intensity of matrix multiplication is too large as $O(N)$, the performance improved near 16 times (AVX2 x four-core) by utilizing both AVX2 and OpenMP in this way.

\begin{table}[htbp]
  \centering
  \caption{Execution time, Speed-up ratio, Performance, Upper bound of performance, and performance level for $j$-$k$-$i$ order of matrix multiplication for $N$ = 2,500}
  \label{tabMM}
  \small
  \begin{tabular}{l|rrrrr}
    \hline
               & \begin{tabular}[c]{@{}c@{}}Execution\\ Time {[}sec{]}\end{tabular} & Speed-up & \begin{tabular}[c]{@{}c@{}}Performance\\ {[}Gflops/sec{]}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Upper bound of\\ Performance\\ {[}Gflops/sec{]}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Performance \\ level {[}\%{]}\end{tabular} \\ \hline
  Serial       & 88.78                                                              & 1.00     & 3.17                                                                   & 5.8                                                                                     & 55                                                                    \\ \hline
  AVX2         & 22.93                                                              & 3.87     & 12.27                                                                  & 23.2                                                                                    & 53                                                                    \\ \hline
  OpenMP       & 22.95                                                              & 3.87     & 12.26                                                                  & 23.2                                                                                    & 53                                                                    \\ \hline
\begin{tabular}[c]{@{}l@{}}AVX2 \&\\ OpenMP\end{tabular} & 6.54                                                               & 13.57    & 42.99                                                                  & 92.8                                                                                    & 46               \\ \hline                                                    
  \end{tabular}
  \end{table}

\section{IEEE-style TODO. 主張をまとめ, 別紙で取捨の確認(パワポp46, p47あたり)}

\subsection{Summary TODO. 文章の修正}

Before offloading to outer C function, it is required to allocate memory address in MATLAB. 
This overhead degrades some vector operations such as vector addition, scalar multiplication of vector, and axpy, which require output of vector. 
This overhead cannot consider in the roof line model, so the performance levels are low. 
Since the inner product is not require output of vector, its performance level is high when using the roof line model. 
However, the performances for vector operations are low or modest, because their operational intensities are low, and being bounded on memory references.

As for DD matrix-vector operations, it is necessary for achieving high performance to use AVX2 loading / storing instructions instead of the set of scalar instructions. 
Using AVX2 in this way, and using OpenMP to the outer loop, the performance can become the 66\% of the upper bound of performance.

For DD matrix multiplication, $j$-$k$-$i$ order of index, using AVX2 to load four continuous data, and using OpenMP to the outer loop is the best for MATLAB.
At that condition, the performance increases almost 16 times and the execution time is reduced, because its operational intensity is so high.

\section{Conclusion TODO. 文書の修正}
In response to demands for ways to facilitate high-precision arithmetic with an interactive computing environment, 
we developed MuPAT on Scilab/MATLAB. MuPAT uses DD arithmetic that requires large number of floating-point operations. 
Executing DD arithmetic takes much execution time due to heavy computation.
It is possible to offload to outer C function by MATLAB executable file and accelerate by FMA, AVX2, OpenMP which are supported by modern CPU. 

Since FMA can reduce the number of floating-point operations, we assume to use FMA.  
%For vector addition and scalar multiplication of vector, the operational intensities are low and the performance is not increased. Operational intensity of inner product is higher than these two vector operations and the performance levels become almost the upper bound of roofline model.
For matrix-vector multiplication and matrix multiplication, there is some way to use AVX2 and OpenMP. 
For AVX2, using vector loading instruction was the key for performance. 
For OpenMP, performance is depending on the combinations of the order of index and the loop to parallelize.



%The performance levels of matrix-vector multiplcation become almost the upper bound of roofline model. That of matrix multiplication is lower, i.e. 60\%. 

The inner products, matrix-vector multiplication, and matrix multiplication require the one multiply-and-add operation.
MuPAT on Scilab is implemented these three operations as one matrix multiplication routine. 
Since the way of implementation to increase the performance for these three operations are different as discribed in the paper, 
MuPAT on MATLAB is implemented as different routines. 
%For matrix-vector multiplication and vector operations, the operational intensities are not enough to speed up theoretically when using AVX2 and OpenMP. 

The effectiveness of AVX2 and OpenMP is depending on the operational intensity. 
As the case of vector addition and axpy, or Cray-style and IEEE-sytle of matrix-vector multiplication, when the operation is being bounded on memory and the number of memory references are the same, 
the execution time become almost the same even the number of floating-point instruction is different. 
When the operational intensity is high, the effectiveness of AVX2 and OpenMP is also high for the operations that is being bouned on memory.

% Even the DD arithmetic, matrix and vector operations are bounded on the memory performance except for matrix multiplication. Recent CPUs can hide the computational workload of these operations in DD arithmetic by using FMA, AVX2, and OpenMP. The next problem for some matrix and vector operations in DD arithmetic is the number of memory references. 

\section*{Acknowledgment}

This research was supported by a grant from the Japan Society for the Promotion of Science (JSPS$\colon$JP17K00164).
\begin{thebibliography}{99}
\bibitem{QD} Y. Hida, X. S. Li, and D. H. Baily, QD arithmetic: algorithms, implementation, and application, Technical Report LBNL-46996, Lawrence Berkeley National Laboratory, 2000.

\bibitem{saito} S. Kikkawa, T. Saito, E. Ishiwata, and H. Hasegawa, Development and acceleration of multiple precision arithmetic toolbox MuPAT for Scilab, {\it JSIAM Letters} {\bf 5} (2013), 9-12. 

\bibitem{hota} MuPAT on MATLAB. {\tt https://www.ed.tus.ac.jp/1419521/index.html}

\bibitem{DD} T. J. Dekker, A floating-point technique for extending the available precision, {\it Numerische Mathematik} {\bf 18} (1971), 224-242. 
\bibitem{SIMD} Intel Intrinsics Guide. {\tt https://software.intel.com/sites/landingpage/IntrinsicsGuide/}
\bibitem{omp} L. Dagum, and R. Menon, OpenMP: An Industry-Standard API for Shared-Memory Programming, {\it IEEE Computational Science \& Engineering} {\bf 5} (1998), 46-55. 

\bibitem{ichi}I. Yamazaki, S. Tomov, T. Dong, J. Dongarra, Mixed-precision orthogonalization scheme and adaptive step size for improving the stability and performance of CA-GMRES on GPUs, in: Proceedings of International Meeting on High Performance Computing for Computational Science (VECPAR), 2014, 17-30.

\bibitem{DDBLAS} X. S. Li et al., Design, Implementation and Testing of Extended and Mixed Precision BLAS, {\it ACM Transactions on Mathematical Software} {\bf 28} (2002), 152-205.

\bibitem{HPC} K. Dowd and C. Severance, High Performance Computing, 2nd Edition, {\it O'Reilly}, 1998.

\bibitem{roof} S. Williams, A. Waterman, and D. Patterson, Roofline: An insightful visual performance model for multicore architectures, {\it Communications of the ACM} {\bf 52} (2009),  65-76.

\bibitem{goto} K. Goto, and R. van de Geijn, Anatomy of High-Performance Matrix Multiplication, {\it ACM Transactions on Mathematical Software} {\bf 34} (2008), Article 12 (25 pages).

\end{thebibliography}
\end{document}